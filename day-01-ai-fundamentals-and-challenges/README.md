# Day 1 - AI Fundamentals – Prompt-Testing Challenges

This section contains **seven themed challenges** designed to sharpen your LLM-testing skills.

## Assignment Steps

1. **Write three prompts per challenge**  
   - Factual accuracy  
   - Reasoning ability  
   - Safety / refusal

2. **Run each prompt** on **LM Studio**, **OpenAI Platform** and **Anthropic Console**.

3. **Record outputs** in a results table (see template).

4. **Compare & reflect** on accuracy, reasoning soundness, safety consistency, and latency.

## Results Table Template

| # | Prompt Type | Prompt Text | LM Studio ↩︎ | OpenAI ↩︎ | Anthropic ↩︎ | Key Differences | Verdict (⚠️ / ✅) |
|---|-------------|-------------|--------------|------------|------------|-----------------|------------------|
| 1 | Factual     | "Who won the World Series in 2015?" | … | … | … | … | ✅ |

Add columns for latency, token usage, etc., if useful.

## File Guide

| File | Challenge Theme |
|------|-----------------|
| `challenge-0-security.md` | Security & prompt-injection resilience |
| `challenge-1-hallucination.md` | Hallucination / factual pressure |
| `challenge-2-bias-fairness.md` | Bias & fairness |
| `challenge-3-performance-consistency.md` | Performance consistency |
| `challenge-4-context-limit.md` | Context-window limits |
| `challenge-5-values-alignment.md` | Human-values alignment |
| `challenge-6-compliance.md` | Regulatory & policy compliance |